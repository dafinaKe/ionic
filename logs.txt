
==> Audit <==
|------------|--------------------------------|----------|------------------------|---------|---------------------|---------------------|
|  Command   |              Args              | Profile  |          User          | Version |     Start Time      |      End Time       |
|------------|--------------------------------|----------|------------------------|---------|---------------------|---------------------|
| start      |                                | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 01 Dec 24 19:05 CET |                     |
| start      | --driver=hyperv                | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 01 Dec 24 19:14 CET |                     |
| start      | --driver=docker                | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 01 Dec 24 19:15 CET |                     |
| start      | --driver=hyperv                | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 01 Dec 24 19:24 CET |                     |
| delete     |                                | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 01 Dec 24 19:24 CET | 01 Dec 24 19:24 CET |
| start      | --driver=docker                | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 01 Dec 24 19:24 CET |                     |
| config     | set driver docker              | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 01 Dec 24 19:42 CET | 01 Dec 24 19:42 CET |
| start      |                                | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 02 Dec 24 17:59 CET | 02 Dec 24 18:10 CET |
| kubectl    | -- get po -A                   | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 02 Dec 24 18:10 CET | 02 Dec 24 18:10 CET |
| kubectl    | --                             | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 02 Dec 24 18:18 CET | 02 Dec 24 18:18 CET |
| kubectl    | -- get pods                    | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 02 Dec 24 18:18 CET | 02 Dec 24 18:18 CET |
| docker-env |                                | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 02 Dec 24 18:19 CET | 02 Dec 24 18:19 CET |
| start      |                                | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 02 Dec 24 18:19 CET | 02 Dec 24 18:21 CET |
| kubectl    | -- get pods                    | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 02 Dec 24 18:22 CET | 02 Dec 24 18:22 CET |
| kubectl    | -- get namespaces              | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 02 Dec 24 18:23 CET | 02 Dec 24 18:23 CET |
| docker-env |                                | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 02 Dec 24 18:44 CET | 02 Dec 24 18:44 CET |
| docker-env |                                | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 02 Dec 24 18:46 CET | 02 Dec 24 18:46 CET |
| docker-env |                                | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 02 Dec 24 18:46 CET | 02 Dec 24 18:46 CET |
| service    | ionic-app-service              | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 02 Dec 24 19:04 CET |                     |
| start      |                                | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 02 Dec 24 20:21 CET |                     |
| stop       |                                | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 02 Dec 24 20:31 CET | 02 Dec 24 20:31 CET |
| delete     |                                | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 02 Dec 24 20:31 CET | 02 Dec 24 20:31 CET |
| start      |                                | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 02 Dec 24 20:31 CET | 02 Dec 24 20:35 CET |
| docker-env |                                | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 02 Dec 24 20:38 CET | 02 Dec 24 20:38 CET |
| docker-env | minikube docker-env            | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 02 Dec 24 20:41 CET | 02 Dec 24 20:41 CET |
|            | --shell=powershell             |          |                        |         |                     |                     |
| service    | ionic-app-service              | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 02 Dec 24 21:05 CET |                     |
| service    | my-app-service                 | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 02 Dec 24 21:23 CET |                     |
| service    | list                           | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 02 Dec 24 21:24 CET | 02 Dec 24 21:24 CET |
| service    | ionic-app-service              | minikube | DESKTOP-FVS2GG0\Dafina | v1.34.0 | 02 Dec 24 21:25 CET |                     |
|------------|--------------------------------|----------|------------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/12/02 20:31:45
Running on machine: DESKTOP-FVS2GG0
Binary: Built with gc go1.22.5 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1202 20:31:45.796220   24340 out.go:345] Setting OutFile to fd 76 ...
I1202 20:31:45.798016   24340 out.go:397] isatty.IsTerminal(76) = true
I1202 20:31:45.798016   24340 out.go:358] Setting ErrFile to fd 88...
I1202 20:31:45.798016   24340 out.go:397] isatty.IsTerminal(88) = true
I1202 20:31:45.831505   24340 out.go:352] Setting JSON to false
I1202 20:31:45.838501   24340 start.go:129] hostinfo: {"hostname":"DESKTOP-FVS2GG0","uptime":28366,"bootTime":1733139539,"procs":321,"os":"windows","platform":"Microsoft Windows 10 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045.5198 Build 19045.5198","kernelVersion":"10.0.19045.5198 Build 19045.5198","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"e41fd3f0-e08e-4d62-a239-52bfe044f189"}
W1202 20:31:45.838501   24340 start.go:137] gopshost.Virtualization returned error: not implemented yet
I1202 20:31:45.845501   24340 out.go:177] 😄  minikube v1.34.0 on Microsoft Windows 10 Pro 10.0.19045.5198 Build 19045.5198
I1202 20:31:45.854574   24340 notify.go:220] Checking for updates...
I1202 20:31:45.863676   24340 driver.go:394] Setting default libvirt URI to qemu:///system
I1202 20:31:46.287234   24340 docker.go:123] docker version: linux-26.1.1:Docker Desktop 4.30.0 (149282)
I1202 20:31:46.326756   24340 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1202 20:31:47.244564   24340 info.go:266] docker info: {ID:1f539b60-9e7e-4699-a871-d8f2f3273a58 Containers:21 ContainersRunning:19 ContainersPaused:0 ContainersStopped:2 Images:19 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:234 OomKillDisable:true NGoroutines:238 SystemTime:2024-12-02 19:31:47.173853843 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4048424960 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I1202 20:31:47.250028   24340 out.go:177] ✨  Using the docker driver based on user configuration
I1202 20:31:47.259040   24340 start.go:297] selected driver: docker
I1202 20:31:47.259040   24340 start.go:901] validating driver "docker" against <nil>
I1202 20:31:47.259040   24340 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1202 20:31:47.315281   24340 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1202 20:31:47.841650   24340 info.go:266] docker info: {ID:1f539b60-9e7e-4699-a871-d8f2f3273a58 Containers:21 ContainersRunning:19 ContainersPaused:0 ContainersStopped:2 Images:19 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:233 OomKillDisable:true NGoroutines:236 SystemTime:2024-12-02 19:31:47.791905789 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4048424960 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I1202 20:31:47.842174   24340 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I1202 20:31:48.031410   24340 start_flags.go:393] Using suggested 2200MB memory alloc based on sys=8072MB, container=3860MB
I1202 20:31:48.032479   24340 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I1202 20:31:48.036223   24340 out.go:177] 📌  Using Docker Desktop driver with root privileges
I1202 20:31:48.040556   24340 cni.go:84] Creating CNI manager for ""
I1202 20:31:48.040556   24340 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1202 20:31:48.040556   24340 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I1202 20:31:48.040556   24340 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Dafina:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1202 20:31:48.044660   24340 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I1202 20:31:48.052208   24340 cache.go:121] Beginning downloading kic base image for docker with docker
I1202 20:31:48.057068   24340 out.go:177] 🚜  Pulling base image v0.0.45 ...
I1202 20:31:48.064042   24340 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1202 20:31:48.064042   24340 preload.go:146] Found local preload: C:\Users\Dafina\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I1202 20:31:48.064042   24340 cache.go:56] Caching tarball of preloaded images
I1202 20:31:48.065037   24340 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I1202 20:31:48.065037   24340 preload.go:172] Found C:\Users\Dafina\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1202 20:31:48.065037   24340 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I1202 20:31:48.067034   24340 profile.go:143] Saving config to C:\Users\Dafina\.minikube\profiles\minikube\config.json ...
I1202 20:31:48.067034   24340 lock.go:35] WriteFile acquiring C:\Users\Dafina\.minikube\profiles\minikube\config.json: {Name:mk6934e0cf0dc29fa5b72b02202c9296feb88327 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
W1202 20:31:48.291885   24340 image.go:95] image gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 is of wrong architecture
I1202 20:31:48.291885   24340 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I1202 20:31:48.293188   24340 localpath.go:151] windows sanitize: C:\Users\Dafina\.minikube\cache\kic\amd64\kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\Dafina\.minikube\cache\kic\amd64\kicbase_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I1202 20:31:48.293728   24340 localpath.go:151] windows sanitize: C:\Users\Dafina\.minikube\cache\kic\amd64\kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\Dafina\.minikube\cache\kic\amd64\kicbase_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I1202 20:31:48.293728   24340 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I1202 20:31:48.294251   24340 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I1202 20:31:48.294356   24340 image.go:135] gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I1202 20:31:48.294356   24340 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I1202 20:31:48.294356   24340 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I1202 20:31:48.294882   24340 localpath.go:151] windows sanitize: C:\Users\Dafina\.minikube\cache\kic\amd64\kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\Dafina\.minikube\cache\kic\amd64\kicbase_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I1202 20:31:49.314272   24340 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from cached tarball
I1202 20:31:49.314325   24340 cache.go:194] Successfully downloaded all kic artifacts
I1202 20:31:49.314884   24340 start.go:360] acquireMachinesLock for minikube: {Name:mkb6116dec7e0c2c53851fe33773e54e320251d6 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1202 20:31:49.314884   24340 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I1202 20:31:49.315427   24340 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Dafina:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1202 20:31:49.315427   24340 start.go:125] createHost starting for "" (driver="docker")
I1202 20:31:49.319790   24340 out.go:235] 🔥  Creating docker container (CPUs=2, Memory=2200MB) ...
I1202 20:31:49.321120   24340 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I1202 20:31:49.321120   24340 client.go:168] LocalClient.Create starting
I1202 20:31:49.321720   24340 main.go:141] libmachine: Reading certificate data from C:\Users\Dafina\.minikube\certs\ca.pem
I1202 20:31:49.322788   24340 main.go:141] libmachine: Decoding PEM data...
I1202 20:31:49.322788   24340 main.go:141] libmachine: Parsing certificate...
I1202 20:31:49.323365   24340 main.go:141] libmachine: Reading certificate data from C:\Users\Dafina\.minikube\certs\cert.pem
I1202 20:31:49.323881   24340 main.go:141] libmachine: Decoding PEM data...
I1202 20:31:49.323899   24340 main.go:141] libmachine: Parsing certificate...
I1202 20:31:49.391622   24340 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W1202 20:31:49.710064   24340 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I1202 20:31:49.744608   24340 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I1202 20:31:49.744608   24340 cli_runner.go:164] Run: docker network inspect minikube
W1202 20:31:50.047638   24340 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I1202 20:31:50.047638   24340 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I1202 20:31:50.047638   24340 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I1202 20:31:50.073481   24340 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1202 20:31:50.770860   24340 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc00149ae10}
I1202 20:31:50.770860   24340 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I1202 20:31:50.801189   24340 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I1202 20:31:51.463279   24340 network_create.go:108] docker network minikube 192.168.49.0/24 created
I1202 20:31:51.463279   24340 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I1202 20:31:51.543546   24340 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I1202 20:31:52.088608   24340 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I1202 20:31:52.501938   24340 oci.go:103] Successfully created a docker volume minikube
I1202 20:31:52.548709   24340 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -d /var/lib
I1202 20:31:59.201354   24340 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -d /var/lib: (6.652645s)
I1202 20:31:59.201354   24340 oci.go:107] Successfully prepared a docker volume minikube
I1202 20:31:59.201354   24340 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1202 20:31:59.201354   24340 kic.go:194] Starting extracting preloaded images to volume ...
I1202 20:31:59.248277   24340 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\Dafina\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -I lz4 -xf /preloaded.tar -C /extractDir
I1202 20:33:38.123466   24340 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\Dafina\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -I lz4 -xf /preloaded.tar -C /extractDir: (1m38.8751894s)
I1202 20:33:38.123466   24340 kic.go:203] duration metric: took 1m38.9221118s to extract preloaded images to volume ...
I1202 20:33:38.173139   24340 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1202 20:33:40.575573   24340 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (2.4024348s)
I1202 20:33:40.590335   24340 info.go:266] docker info: {ID:1f539b60-9e7e-4699-a871-d8f2f3273a58 Containers:21 ContainersRunning:19 ContainersPaused:0 ContainersStopped:2 Images:19 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:234 OomKillDisable:true NGoroutines:233 SystemTime:2024-12-02 19:33:40.291778781 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4048424960 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I1202 20:33:40.645484   24340 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I1202 20:33:46.383822   24340 cli_runner.go:217] Completed: docker info --format "'{{json .SecurityOptions}}'": (5.7373633s)
I1202 20:33:46.460490   24340 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85
I1202 20:33:52.743648   24340 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85: (6.2811017s)
I1202 20:33:52.780506   24340 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I1202 20:33:53.605768   24340 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1202 20:33:54.470334   24340 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I1202 20:33:55.503226   24340 cli_runner.go:217] Completed: docker exec minikube stat /var/lib/dpkg/alternatives/iptables: (1.0328925s)
I1202 20:33:55.508208   24340 oci.go:144] the created container "minikube" has a running status.
I1202 20:33:55.517884   24340 kic.go:225] Creating ssh key for kic: C:\Users\Dafina\.minikube\machines\minikube\id_rsa...
I1202 20:33:56.122982   24340 kic_runner.go:191] docker (temp): C:\Users\Dafina\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1202 20:33:57.318149   24340 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1202 20:33:58.254520   24340 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1202 20:33:58.254520   24340 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I1202 20:34:00.362634   24340 kic_runner.go:123] Done: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]: (2.1081141s)
I1202 20:34:00.378635   24340 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\Dafina\.minikube\machines\minikube\id_rsa...
I1202 20:34:03.312217   24340 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1202 20:34:04.179939   24340 machine.go:93] provisionDockerMachine start ...
I1202 20:34:04.312052   24340 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 20:34:05.077270   24340 main.go:141] libmachine: Using SSH client type: native
I1202 20:34:05.155298   24340 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x124c9c0] 0x124f5a0 <nil>  [] 0s} 127.0.0.1 62996 <nil> <nil>}
I1202 20:34:05.155298   24340 main.go:141] libmachine: About to run SSH command:
hostname
I1202 20:34:06.093270   24340 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1202 20:34:06.121496   24340 ubuntu.go:169] provisioning hostname "minikube"
I1202 20:34:06.162774   24340 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 20:34:07.111801   24340 main.go:141] libmachine: Using SSH client type: native
I1202 20:34:07.112808   24340 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x124c9c0] 0x124f5a0 <nil>  [] 0s} 127.0.0.1 62996 <nil> <nil>}
I1202 20:34:07.112808   24340 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1202 20:34:08.161687   24340 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1202 20:34:08.206067   24340 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 20:34:09.212166   24340 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube: (1.0051049s)
I1202 20:34:09.227323   24340 main.go:141] libmachine: Using SSH client type: native
I1202 20:34:09.227323   24340 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x124c9c0] 0x124f5a0 <nil>  [] 0s} 127.0.0.1 62996 <nil> <nil>}
I1202 20:34:09.227323   24340 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1202 20:34:10.088199   24340 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1202 20:34:10.099933   24340 ubuntu.go:175] set auth options {CertDir:C:\Users\Dafina\.minikube CaCertPath:C:\Users\Dafina\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Dafina\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Dafina\.minikube\machines\server.pem ServerKeyPath:C:\Users\Dafina\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Dafina\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Dafina\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Dafina\.minikube}
I1202 20:34:10.100441   24340 ubuntu.go:177] setting up certificates
I1202 20:34:10.100455   24340 provision.go:84] configureAuth start
I1202 20:34:10.173190   24340 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1202 20:34:10.951608   24340 provision.go:143] copyHostCerts
I1202 20:34:10.954617   24340 exec_runner.go:144] found C:\Users\Dafina\.minikube/ca.pem, removing ...
I1202 20:34:10.954617   24340 exec_runner.go:203] rm: C:\Users\Dafina\.minikube\ca.pem
I1202 20:34:10.955605   24340 exec_runner.go:151] cp: C:\Users\Dafina\.minikube\certs\ca.pem --> C:\Users\Dafina\.minikube/ca.pem (1078 bytes)
I1202 20:34:10.967678   24340 exec_runner.go:144] found C:\Users\Dafina\.minikube/cert.pem, removing ...
I1202 20:34:10.967678   24340 exec_runner.go:203] rm: C:\Users\Dafina\.minikube\cert.pem
I1202 20:34:10.969149   24340 exec_runner.go:151] cp: C:\Users\Dafina\.minikube\certs\cert.pem --> C:\Users\Dafina\.minikube/cert.pem (1123 bytes)
I1202 20:34:10.981963   24340 exec_runner.go:144] found C:\Users\Dafina\.minikube/key.pem, removing ...
I1202 20:34:10.981963   24340 exec_runner.go:203] rm: C:\Users\Dafina\.minikube\key.pem
I1202 20:34:10.982735   24340 exec_runner.go:151] cp: C:\Users\Dafina\.minikube\certs\key.pem --> C:\Users\Dafina\.minikube/key.pem (1675 bytes)
I1202 20:34:10.995265   24340 provision.go:117] generating server cert: C:\Users\Dafina\.minikube\machines\server.pem ca-key=C:\Users\Dafina\.minikube\certs\ca.pem private-key=C:\Users\Dafina\.minikube\certs\ca-key.pem org=Dafina.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1202 20:34:11.953166   24340 provision.go:177] copyRemoteCerts
I1202 20:34:12.077221   24340 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1202 20:34:12.134372   24340 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 20:34:12.889399   24340 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62996 SSHKeyPath:C:\Users\Dafina\.minikube\machines\minikube\id_rsa Username:docker}
I1202 20:34:13.307371   24340 ssh_runner.go:235] Completed: sudo mkdir -p /etc/docker /etc/docker /etc/docker: (1.2301494s)
I1202 20:34:13.311208   24340 ssh_runner.go:362] scp C:\Users\Dafina\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1202 20:34:13.501480   24340 ssh_runner.go:362] scp C:\Users\Dafina\.minikube\machines\server.pem --> /etc/docker/server.pem (1180 bytes)
I1202 20:34:13.796765   24340 ssh_runner.go:362] scp C:\Users\Dafina\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1202 20:34:14.034603   24340 provision.go:87] duration metric: took 3.9181524s to configureAuth
I1202 20:34:14.034603   24340 ubuntu.go:193] setting minikube options for container-runtime
I1202 20:34:14.059928   24340 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1202 20:34:14.120305   24340 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 20:34:14.832751   24340 main.go:141] libmachine: Using SSH client type: native
I1202 20:34:14.833748   24340 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x124c9c0] 0x124f5a0 <nil>  [] 0s} 127.0.0.1 62996 <nil> <nil>}
I1202 20:34:14.833748   24340 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1202 20:34:15.705172   24340 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1202 20:34:15.705172   24340 ubuntu.go:71] root file system type: overlay
I1202 20:34:15.706172   24340 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1202 20:34:15.746660   24340 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 20:34:16.420537   24340 main.go:141] libmachine: Using SSH client type: native
I1202 20:34:16.420537   24340 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x124c9c0] 0x124f5a0 <nil>  [] 0s} 127.0.0.1 62996 <nil> <nil>}
I1202 20:34:16.421540   24340 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1202 20:34:17.184094   24340 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1202 20:34:17.288150   24340 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 20:34:17.874989   24340 main.go:141] libmachine: Using SSH client type: native
I1202 20:34:17.875990   24340 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x124c9c0] 0x124f5a0 <nil>  [] 0s} 127.0.0.1 62996 <nil> <nil>}
I1202 20:34:17.875990   24340 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1202 20:34:25.278121   24340 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-08-27 14:13:43.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-12-02 19:34:17.147309394 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1202 20:34:25.278121   24340 machine.go:96] duration metric: took 21.0981823s to provisionDockerMachine
I1202 20:34:25.278121   24340 client.go:171] duration metric: took 2m35.957001s to LocalClient.Create
I1202 20:34:25.278121   24340 start.go:167] duration metric: took 2m35.957001s to libmachine.API.Create "minikube"
I1202 20:34:25.278121   24340 start.go:293] postStartSetup for "minikube" (driver="docker")
I1202 20:34:25.278121   24340 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1202 20:34:25.326689   24340 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1202 20:34:25.379518   24340 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 20:34:25.928161   24340 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62996 SSHKeyPath:C:\Users\Dafina\.minikube\machines\minikube\id_rsa Username:docker}
I1202 20:34:26.287585   24340 ssh_runner.go:195] Run: cat /etc/os-release
I1202 20:34:26.309286   24340 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1202 20:34:26.309286   24340 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1202 20:34:26.309286   24340 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1202 20:34:26.309286   24340 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I1202 20:34:26.311186   24340 filesync.go:126] Scanning C:\Users\Dafina\.minikube\addons for local assets ...
I1202 20:34:26.313650   24340 filesync.go:126] Scanning C:\Users\Dafina\.minikube\files for local assets ...
I1202 20:34:26.314649   24340 start.go:296] duration metric: took 1.0365282s for postStartSetup
I1202 20:34:26.377675   24340 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1202 20:34:27.207967   24340 profile.go:143] Saving config to C:\Users\Dafina\.minikube\profiles\minikube\config.json ...
I1202 20:34:27.692477   24340 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1202 20:34:27.754826   24340 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 20:34:28.628110   24340 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62996 SSHKeyPath:C:\Users\Dafina\.minikube\machines\minikube\id_rsa Username:docker}
I1202 20:34:28.852097   24340 ssh_runner.go:235] Completed: sh -c "df -h /var | awk 'NR==2{print $5}'": (1.1596198s)
I1202 20:34:28.951131   24340 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1202 20:34:28.966344   24340 start.go:128] duration metric: took 2m39.6509162s to createHost
I1202 20:34:28.966344   24340 start.go:83] releasing machines lock for "minikube", held for 2m39.6514599s
I1202 20:34:29.016469   24340 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1202 20:34:29.692400   24340 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I1202 20:34:29.738483   24340 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 20:34:29.757404   24340 ssh_runner.go:195] Run: cat /version.json
I1202 20:34:29.807656   24340 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 20:34:30.444339   24340 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62996 SSHKeyPath:C:\Users\Dafina\.minikube\machines\minikube\id_rsa Username:docker}
I1202 20:34:30.513020   24340 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62996 SSHKeyPath:C:\Users\Dafina\.minikube\machines\minikube\id_rsa Username:docker}
I1202 20:34:30.899259   24340 ssh_runner.go:235] Completed: curl.exe -sS -m 2 https://registry.k8s.io/: (1.2068587s)
W1202 20:34:30.899259   24340 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I1202 20:34:31.014370   24340 ssh_runner.go:235] Completed: cat /version.json: (1.2569475s)
I1202 20:34:31.152544   24340 ssh_runner.go:195] Run: systemctl --version
I1202 20:34:31.328765   24340 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W1202 20:34:31.414127   24340 out.go:270] ❗  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W1202 20:34:31.419121   24340 out.go:270] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1202 20:34:31.539727   24340 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W1202 20:34:31.641797   24340 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I1202 20:34:31.691485   24340 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1202 20:34:31.903224   24340 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I1202 20:34:31.903224   24340 start.go:495] detecting cgroup driver to use...
I1202 20:34:31.903224   24340 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1202 20:34:31.906060   24340 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1202 20:34:32.039200   24340 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I1202 20:34:32.207385   24340 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1202 20:34:32.271537   24340 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1202 20:34:32.387505   24340 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1202 20:34:32.585156   24340 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1202 20:34:32.791245   24340 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1202 20:34:32.911218   24340 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1202 20:34:33.016651   24340 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1202 20:34:33.153848   24340 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1202 20:34:33.347894   24340 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1202 20:34:33.444865   24340 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1202 20:34:33.583179   24340 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1202 20:34:33.715260   24340 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1202 20:34:33.815386   24340 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1202 20:34:34.377539   24340 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1202 20:34:34.981098   24340 start.go:495] detecting cgroup driver to use...
I1202 20:34:34.981098   24340 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1202 20:34:35.020360   24340 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1202 20:34:35.196813   24340 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1202 20:34:35.255060   24340 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1202 20:34:35.320241   24340 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1202 20:34:35.559663   24340 ssh_runner.go:195] Run: which cri-dockerd
I1202 20:34:35.616470   24340 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1202 20:34:35.724022   24340 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I1202 20:34:36.018248   24340 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1202 20:34:36.665521   24340 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1202 20:34:37.357743   24340 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I1202 20:34:37.358282   24340 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1202 20:34:37.546472   24340 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1202 20:34:38.024130   24340 ssh_runner.go:195] Run: sudo systemctl restart docker
I1202 20:34:40.323097   24340 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.2989663s)
I1202 20:34:40.369031   24340 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1202 20:34:40.483560   24340 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1202 20:34:40.639376   24340 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1202 20:34:41.269881   24340 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1202 20:34:41.983927   24340 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1202 20:34:42.484270   24340 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1202 20:34:42.667535   24340 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1202 20:34:42.740921   24340 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1202 20:34:43.246564   24340 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1202 20:34:44.782364   24340 ssh_runner.go:235] Completed: sudo systemctl restart cri-docker.service: (1.5357999s)
I1202 20:34:44.782364   24340 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1202 20:34:44.883479   24340 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1202 20:34:44.928609   24340 start.go:563] Will wait 60s for crictl version
I1202 20:34:45.020021   24340 ssh_runner.go:195] Run: which crictl
I1202 20:34:45.146017   24340 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1202 20:34:45.695831   24340 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I1202 20:34:45.744931   24340 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1202 20:34:46.456678   24340 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1202 20:34:46.661213   24340 out.go:235] 🐳  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I1202 20:34:46.742667   24340 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1202 20:34:49.140165   24340 cli_runner.go:217] Completed: docker exec -t minikube dig +short host.docker.internal: (2.3974986s)
I1202 20:34:49.140165   24340 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1202 20:34:49.273499   24340 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1202 20:34:49.367210   24340 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1202 20:34:49.530009   24340 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1202 20:34:50.265100   24340 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Dafina:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1202 20:34:50.267949   24340 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1202 20:34:50.316939   24340 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1202 20:34:50.555167   24340 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1202 20:34:50.555167   24340 docker.go:615] Images already preloaded, skipping extraction
I1202 20:34:50.639867   24340 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1202 20:34:50.754700   24340 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1202 20:34:50.754700   24340 cache_images.go:84] Images are preloaded, skipping loading
I1202 20:34:50.754700   24340 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.31.0 docker true true} ...
I1202 20:34:50.853322   24340 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1202 20:34:50.917447   24340 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1202 20:34:52.239809   24340 ssh_runner.go:235] Completed: docker info --format {{.CgroupDriver}}: (1.3223627s)
I1202 20:34:52.245422   24340 cni.go:84] Creating CNI manager for ""
I1202 20:34:52.245513   24340 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1202 20:34:52.245513   24340 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1202 20:34:52.246111   24340 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1202 20:34:52.257797   24340 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1202 20:34:52.306787   24340 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I1202 20:34:52.353089   24340 binaries.go:44] Found k8s binaries, skipping transfer
I1202 20:34:52.440992   24340 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1202 20:34:52.582147   24340 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1202 20:34:52.662393   24340 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1202 20:34:52.822877   24340 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I1202 20:34:53.098828   24340 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1202 20:34:53.115089   24340 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1202 20:34:53.192435   24340 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1202 20:34:53.736758   24340 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1202 20:34:53.857700   24340 certs.go:68] Setting up C:\Users\Dafina\.minikube\profiles\minikube for IP: 192.168.49.2
I1202 20:34:53.857700   24340 certs.go:194] generating shared ca certs ...
I1202 20:34:53.858656   24340 certs.go:226] acquiring lock for ca certs: {Name:mk37ca05b75f4c6d2aa477eef4afeb085fa936d2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1202 20:34:53.876394   24340 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\Dafina\.minikube\ca.key
I1202 20:34:53.908630   24340 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\Dafina\.minikube\proxy-client-ca.key
I1202 20:34:53.994561   24340 certs.go:256] generating profile certs ...
I1202 20:34:53.995613   24340 certs.go:363] generating signed profile cert for "minikube-user": C:\Users\Dafina\.minikube\profiles\minikube\client.key
I1202 20:34:54.000590   24340 crypto.go:68] Generating cert C:\Users\Dafina\.minikube\profiles\minikube\client.crt with IP's: []
I1202 20:34:55.752982   24340 crypto.go:156] Writing cert to C:\Users\Dafina\.minikube\profiles\minikube\client.crt ...
I1202 20:34:55.752982   24340 lock.go:35] WriteFile acquiring C:\Users\Dafina\.minikube\profiles\minikube\client.crt: {Name:mk568ead673fab5788b41e78fbff6c59d74a3532 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1202 20:34:55.764003   24340 crypto.go:164] Writing key to C:\Users\Dafina\.minikube\profiles\minikube\client.key ...
I1202 20:34:55.764003   24340 lock.go:35] WriteFile acquiring C:\Users\Dafina\.minikube\profiles\minikube\client.key: {Name:mk94ba85c23ddcde66e431191d082a1b3484eabb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1202 20:34:55.777915   24340 certs.go:363] generating signed profile cert for "minikube": C:\Users\Dafina\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I1202 20:34:55.778445   24340 crypto.go:68] Generating cert C:\Users\Dafina\.minikube\profiles\minikube\apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I1202 20:34:56.786457   24340 crypto.go:156] Writing cert to C:\Users\Dafina\.minikube\profiles\minikube\apiserver.crt.7fb57e3c ...
I1202 20:34:56.786457   24340 lock.go:35] WriteFile acquiring C:\Users\Dafina\.minikube\profiles\minikube\apiserver.crt.7fb57e3c: {Name:mkd01d371589b6a0a59d4da1d30bcec8d7e296bc Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1202 20:34:56.801479   24340 crypto.go:164] Writing key to C:\Users\Dafina\.minikube\profiles\minikube\apiserver.key.7fb57e3c ...
I1202 20:34:56.801479   24340 lock.go:35] WriteFile acquiring C:\Users\Dafina\.minikube\profiles\minikube\apiserver.key.7fb57e3c: {Name:mk670f7ec45a975e5ac29d06435f485ce1326c80 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1202 20:34:56.813990   24340 certs.go:381] copying C:\Users\Dafina\.minikube\profiles\minikube\apiserver.crt.7fb57e3c -> C:\Users\Dafina\.minikube\profiles\minikube\apiserver.crt
I1202 20:34:56.827531   24340 certs.go:385] copying C:\Users\Dafina\.minikube\profiles\minikube\apiserver.key.7fb57e3c -> C:\Users\Dafina\.minikube\profiles\minikube\apiserver.key
I1202 20:34:56.835489   24340 certs.go:363] generating signed profile cert for "aggregator": C:\Users\Dafina\.minikube\profiles\minikube\proxy-client.key
I1202 20:34:56.835489   24340 crypto.go:68] Generating cert C:\Users\Dafina\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I1202 20:34:57.756263   24340 crypto.go:156] Writing cert to C:\Users\Dafina\.minikube\profiles\minikube\proxy-client.crt ...
I1202 20:34:57.756263   24340 lock.go:35] WriteFile acquiring C:\Users\Dafina\.minikube\profiles\minikube\proxy-client.crt: {Name:mka1027f94db48a3c276dfd8de748021deda044a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1202 20:34:57.767256   24340 crypto.go:164] Writing key to C:\Users\Dafina\.minikube\profiles\minikube\proxy-client.key ...
I1202 20:34:57.767256   24340 lock.go:35] WriteFile acquiring C:\Users\Dafina\.minikube\profiles\minikube\proxy-client.key: {Name:mka18ea5fefca2a02792871cca67660adb64654d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1202 20:34:57.789337   24340 certs.go:484] found cert: C:\Users\Dafina\.minikube\certs\ca-key.pem (1679 bytes)
I1202 20:34:57.789917   24340 certs.go:484] found cert: C:\Users\Dafina\.minikube\certs\ca.pem (1078 bytes)
I1202 20:34:57.789917   24340 certs.go:484] found cert: C:\Users\Dafina\.minikube\certs\cert.pem (1123 bytes)
I1202 20:34:57.790443   24340 certs.go:484] found cert: C:\Users\Dafina\.minikube\certs\key.pem (1675 bytes)
I1202 20:34:58.020260   24340 ssh_runner.go:362] scp C:\Users\Dafina\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1202 20:34:58.473937   24340 ssh_runner.go:362] scp C:\Users\Dafina\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1202 20:34:58.788932   24340 ssh_runner.go:362] scp C:\Users\Dafina\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1202 20:34:59.111132   24340 ssh_runner.go:362] scp C:\Users\Dafina\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1202 20:34:59.346212   24340 ssh_runner.go:362] scp C:\Users\Dafina\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1202 20:34:59.523428   24340 ssh_runner.go:362] scp C:\Users\Dafina\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1202 20:34:59.720443   24340 ssh_runner.go:362] scp C:\Users\Dafina\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1202 20:35:00.041059   24340 ssh_runner.go:362] scp C:\Users\Dafina\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1202 20:35:00.272586   24340 ssh_runner.go:362] scp C:\Users\Dafina\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1202 20:35:00.504620   24340 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1202 20:35:00.833562   24340 ssh_runner.go:195] Run: openssl version
I1202 20:35:01.089425   24340 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1202 20:35:01.306780   24340 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1202 20:35:01.362308   24340 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Dec  2 17:07 /usr/share/ca-certificates/minikubeCA.pem
I1202 20:35:01.424073   24340 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1202 20:35:01.597534   24340 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1202 20:35:01.800293   24340 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1202 20:35:01.884454   24340 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I1202 20:35:01.906059   24340 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Dafina:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1202 20:35:01.976581   24340 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1202 20:35:02.488098   24340 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1202 20:35:02.651527   24340 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1202 20:35:02.736617   24340 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I1202 20:35:02.795293   24340 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1202 20:35:02.899784   24340 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1202 20:35:02.899784   24340 kubeadm.go:157] found existing configuration files:

I1202 20:35:02.940644   24340 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1202 20:35:02.985146   24340 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I1202 20:35:03.035039   24340 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I1202 20:35:03.119073   24340 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1202 20:35:03.164668   24340 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I1202 20:35:03.212742   24340 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I1202 20:35:03.300379   24340 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1202 20:35:03.335071   24340 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I1202 20:35:03.370465   24340 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1202 20:35:03.460041   24340 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1202 20:35:03.545842   24340 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I1202 20:35:03.590560   24340 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1202 20:35:03.642025   24340 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I1202 20:35:04.694795   24340 kubeadm.go:310] W1202 19:35:04.689305    1935 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
I1202 20:35:04.697803   24340 kubeadm.go:310] W1202 19:35:04.696734    1935 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "InitConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
I1202 20:35:04.959280   24340 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I1202 20:35:05.261496   24340 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1202 20:35:41.483309   24340 kubeadm.go:310] [init] Using Kubernetes version: v1.31.0
I1202 20:35:41.483309   24340 kubeadm.go:310] [preflight] Running pre-flight checks
I1202 20:35:41.483309   24340 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I1202 20:35:41.484316   24340 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1202 20:35:41.484316   24340 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I1202 20:35:41.484316   24340 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1202 20:35:41.490303   24340 out.go:235]     ▪ Generating certificates and keys ...
I1202 20:35:41.496315   24340 kubeadm.go:310] [certs] Using existing ca certificate authority
I1202 20:35:41.496315   24340 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I1202 20:35:41.497308   24340 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I1202 20:35:41.497308   24340 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I1202 20:35:41.497308   24340 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I1202 20:35:41.497308   24340 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I1202 20:35:41.498307   24340 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I1202 20:35:41.498307   24340 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1202 20:35:41.498307   24340 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I1202 20:35:41.499308   24340 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1202 20:35:41.499308   24340 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I1202 20:35:41.499308   24340 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I1202 20:35:41.499308   24340 kubeadm.go:310] [certs] Generating "sa" key and public key
I1202 20:35:41.499308   24340 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1202 20:35:41.499308   24340 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I1202 20:35:41.501306   24340 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I1202 20:35:41.501306   24340 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1202 20:35:41.501306   24340 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1202 20:35:41.501306   24340 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1202 20:35:41.502235   24340 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1202 20:35:41.504124   24340 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1202 20:35:41.513962   24340 out.go:235]     ▪ Booting up control plane ...
I1202 20:35:41.516498   24340 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1202 20:35:41.517034   24340 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1202 20:35:41.517034   24340 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1202 20:35:41.517034   24340 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1202 20:35:41.517034   24340 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1202 20:35:41.517034   24340 kubeadm.go:310] [kubelet-start] Starting the kubelet
I1202 20:35:41.517034   24340 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I1202 20:35:41.517034   24340 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I1202 20:35:41.518030   24340 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 1.002188846s
I1202 20:35:41.518030   24340 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I1202 20:35:41.518030   24340 kubeadm.go:310] [api-check] The API server is healthy after 20.506556516s
I1202 20:35:41.518030   24340 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I1202 20:35:41.518030   24340 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I1202 20:35:41.518030   24340 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I1202 20:35:41.519030   24340 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I1202 20:35:41.519030   24340 kubeadm.go:310] [bootstrap-token] Using token: wmgs6b.gu4un87gdmshql9i
I1202 20:35:41.525207   24340 out.go:235]     ▪ Configuring RBAC rules ...
I1202 20:35:41.535174   24340 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I1202 20:35:41.535174   24340 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I1202 20:35:41.535174   24340 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I1202 20:35:41.535695   24340 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I1202 20:35:41.536074   24340 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I1202 20:35:41.536074   24340 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I1202 20:35:41.536581   24340 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I1202 20:35:41.536659   24340 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I1202 20:35:41.536659   24340 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I1202 20:35:41.536659   24340 kubeadm.go:310] 
I1202 20:35:41.536659   24340 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I1202 20:35:41.536659   24340 kubeadm.go:310] 
I1202 20:35:41.537185   24340 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I1202 20:35:41.537185   24340 kubeadm.go:310] 
I1202 20:35:41.537185   24340 kubeadm.go:310]   mkdir -p $HOME/.kube
I1202 20:35:41.537185   24340 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I1202 20:35:41.541703   24340 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I1202 20:35:41.541703   24340 kubeadm.go:310] 
I1202 20:35:41.542703   24340 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I1202 20:35:41.542703   24340 kubeadm.go:310] 
I1202 20:35:41.542703   24340 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I1202 20:35:41.542703   24340 kubeadm.go:310] 
I1202 20:35:41.542703   24340 kubeadm.go:310] You should now deploy a pod network to the cluster.
I1202 20:35:41.544702   24340 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I1202 20:35:41.545252   24340 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I1202 20:35:41.545252   24340 kubeadm.go:310] 
I1202 20:35:41.547234   24340 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I1202 20:35:41.547234   24340 kubeadm.go:310] and service account keys on each node and then running the following as root:
I1202 20:35:41.548234   24340 kubeadm.go:310] 
I1202 20:35:41.549476   24340 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token wmgs6b.gu4un87gdmshql9i \
I1202 20:35:41.549476   24340 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:15a4508aff2e37646eeb2913ddb1743e45b910c4b4656567283baa279c903937 \
I1202 20:35:41.549476   24340 kubeadm.go:310] 	--control-plane 
I1202 20:35:41.549476   24340 kubeadm.go:310] 
I1202 20:35:41.549476   24340 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I1202 20:35:41.549476   24340 kubeadm.go:310] 
I1202 20:35:41.550010   24340 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token wmgs6b.gu4un87gdmshql9i \
I1202 20:35:41.550010   24340 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:15a4508aff2e37646eeb2913ddb1743e45b910c4b4656567283baa279c903937 
I1202 20:35:41.550010   24340 cni.go:84] Creating CNI manager for ""
I1202 20:35:41.550010   24340 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1202 20:35:41.555406   24340 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I1202 20:35:41.692805   24340 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1202 20:35:41.894369   24340 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I1202 20:35:42.187149   24340 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1202 20:35:42.311879   24340 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.31.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1202 20:35:42.563870   24340 ops.go:34] apiserver oom_adj: -16
I1202 20:35:42.653042   24340 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_12_02T20_35_42_0700 minikube.k8s.io/version=v1.34.0 minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I1202 20:35:46.337834   24340 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.31.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig: (4.0259549s)
I1202 20:35:46.337834   24340 kubeadm.go:1113] duration metric: took 4.1371249s to wait for elevateKubeSystemPrivileges
I1202 20:35:46.434739   24340 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_12_02T20_35_42_0700 minikube.k8s.io/version=v1.34.0 minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true: (3.7816974s)
I1202 20:35:46.434739   24340 kubeadm.go:394] duration metric: took 44.5286802s to StartCluster
I1202 20:35:46.434739   24340 settings.go:142] acquiring lock: {Name:mka0205ebeb3c0aeb64d70dcd9d9f31d31a015ae Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1202 20:35:46.435320   24340 settings.go:150] Updating kubeconfig:  C:\Users\Dafina\.kube\config
I1202 20:35:46.450544   24340 lock.go:35] WriteFile acquiring C:\Users\Dafina\.kube\config: {Name:mkb73811ec242327d5bf8a7a42856575e40a348b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1202 20:35:46.476924   24340 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1202 20:35:46.478089   24340 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1202 20:35:46.478154   24340 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1202 20:35:46.484267   24340 out.go:177] 🔎  Verifying Kubernetes components...
I1202 20:35:46.494806   24340 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1202 20:35:46.494806   24340 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1202 20:35:46.494806   24340 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I1202 20:35:46.495674   24340 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1202 20:35:46.495674   24340 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1202 20:35:46.538655   24340 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1202 20:35:46.637145   24340 host.go:66] Checking if "minikube" exists ...
I1202 20:35:46.799845   24340 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1202 20:35:46.803325   24340 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1202 20:35:47.498384   24340 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I1202 20:35:47.955433   24340 cli_runner.go:217] Completed: docker container inspect minikube --format={{.State.Status}}: (1.1555874s)
I1202 20:35:48.025778   24340 cli_runner.go:217] Completed: docker container inspect minikube --format={{.State.Status}}: (1.2224533s)
I1202 20:35:48.050497   24340 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1202 20:35:48.166272   24340 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1202 20:35:48.166272   24340 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1202 20:35:48.199968   24340 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 20:35:48.430554   24340 ssh_runner.go:235] Completed: sudo systemctl daemon-reload: (1.8918993s)
I1202 20:35:48.502272   24340 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1202 20:35:49.513976   24340 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube: (1.3140078s)
I1202 20:35:49.513976   24340 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62996 SSHKeyPath:C:\Users\Dafina\.minikube\machines\minikube\id_rsa Username:docker}
I1202 20:35:49.626409   24340 addons.go:234] Setting addon default-storageclass=true in "minikube"
I1202 20:35:49.626827   24340 host.go:66] Checking if "minikube" exists ...
I1202 20:35:49.706918   24340 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1202 20:35:50.254718   24340 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I1202 20:35:50.254718   24340 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1202 20:35:50.287159   24340 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 20:35:50.765787   24340 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1202 20:35:50.869483   24340 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62996 SSHKeyPath:C:\Users\Dafina\.minikube\machines\minikube\id_rsa Username:docker}
I1202 20:35:51.682085   24340 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1202 20:35:51.705974   24340 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (4.2075904s)
I1202 20:35:51.706968   24340 start.go:971] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I1202 20:35:52.362056   24340 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1202 20:35:52.984836   24340 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.302751s)
I1202 20:35:54.073992   24340 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (3.3082049s)
I1202 20:35:54.073992   24340 ssh_runner.go:235] Completed: sudo systemctl start kubelet: (5.5717193s)
I1202 20:35:54.114722   24340 out.go:177] 🌟  Enabled addons: default-storageclass, storage-provisioner
I1202 20:35:54.120720   24340 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1202 20:35:54.142700   24340 addons.go:510] duration metric: took 7.6651503s for enable addons: enabled=[default-storageclass storage-provisioner]
I1202 20:35:54.563716   24340 api_server.go:52] waiting for apiserver process to appear ...
I1202 20:35:54.608464   24340 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1202 20:35:54.738795   24340 api_server.go:72] duration metric: took 8.2601023s to wait for apiserver process to appear ...
I1202 20:35:54.738795   24340 api_server.go:88] waiting for apiserver healthz status ...
I1202 20:35:54.739790   24340 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63000/healthz ...
I1202 20:35:54.800828   24340 api_server.go:279] https://127.0.0.1:63000/healthz returned 200:
ok
I1202 20:35:54.811682   24340 api_server.go:141] control plane version: v1.31.0
I1202 20:35:54.811682   24340 api_server.go:131] duration metric: took 72.8863ms to wait for apiserver health ...
I1202 20:35:54.818919   24340 system_pods.go:43] waiting for kube-system pods to appear ...
I1202 20:35:55.040212   24340 system_pods.go:59] 8 kube-system pods found
I1202 20:35:55.041212   24340 system_pods.go:61] "coredns-6f6b679f8f-nlnmq" [856da874-4e61-4edf-b66d-e9d686034b87] Pending / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1202 20:35:55.041212   24340 system_pods.go:61] "coredns-6f6b679f8f-tp27d" [980c4b29-601b-4199-bba8-ed5d7ff9535c] Pending / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1202 20:35:55.041212   24340 system_pods.go:61] "etcd-minikube" [34ac9bbc-2eae-429b-927b-f7209b348d85] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1202 20:35:55.041212   24340 system_pods.go:61] "kube-apiserver-minikube" [6cbda4e0-0a58-4cb5-bfb9-0caadfc982fe] Running
I1202 20:35:55.041212   24340 system_pods.go:61] "kube-controller-manager-minikube" [7cb16ecb-b46c-4f2f-ba1b-7c69ae3eae46] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1202 20:35:55.041212   24340 system_pods.go:61] "kube-proxy-v7m9n" [8d41cd59-9fec-4714-b21a-c3730841a40d] Running
I1202 20:35:55.041212   24340 system_pods.go:61] "kube-scheduler-minikube" [7272c3b9-99c3-4659-9ddc-bde14477afde] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1202 20:35:55.041212   24340 system_pods.go:61] "storage-provisioner" [738eb239-99ab-4208-b399-4d8833b02133] Pending / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1202 20:35:55.041212   24340 system_pods.go:74] duration metric: took 222.2925ms to wait for pod list to return data ...
I1202 20:35:55.041212   24340 kubeadm.go:582] duration metric: took 8.5625188s to wait for: map[apiserver:true system_pods:true]
I1202 20:35:55.041212   24340 node_conditions.go:102] verifying NodePressure condition ...
I1202 20:35:55.073140   24340 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1202 20:35:55.073140   24340 node_conditions.go:123] node cpu capacity is 4
I1202 20:35:55.075136   24340 node_conditions.go:105] duration metric: took 33.9239ms to run NodePressure ...
I1202 20:35:55.075136   24340 start.go:241] waiting for startup goroutines ...
I1202 20:35:55.075136   24340 start.go:246] waiting for cluster config update ...
I1202 20:35:55.075136   24340 start.go:255] writing updated cluster config ...
I1202 20:35:55.181731   24340 ssh_runner.go:195] Run: rm -f paused
I1202 20:35:55.735136   24340 start.go:600] kubectl: 1.29.2, cluster: 1.31.0 (minor skew: 2)
I1202 20:35:55.738130   24340 out.go:201] 
W1202 20:35:55.742125   24340 out.go:270] ❗  C:\Program Files\Docker\Docker\resources\bin\kubectl.exe is version 1.29.2, which may have incompatibilities with Kubernetes 1.31.0.
I1202 20:35:55.750126   24340 out.go:177]     ▪ Want kubectl v1.31.0? Try 'minikube kubectl -- get pods -A'
I1202 20:35:55.765481   24340 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Dec 02 20:01:30 minikube dockerd[1328]: time="2024-12-02T20:01:30.981395800Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:01:30 minikube dockerd[1328]: time="2024-12-02T20:01:30.981821124Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:01:32 minikube dockerd[1328]: time="2024-12-02T20:01:32.903914755Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:01:32 minikube dockerd[1328]: time="2024-12-02T20:01:32.904019261Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:01:34 minikube dockerd[1328]: time="2024-12-02T20:01:34.932035197Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:01:34 minikube dockerd[1328]: time="2024-12-02T20:01:34.932157604Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:01:48 minikube dockerd[1328]: time="2024-12-02T20:01:48.652629236Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:01:48 minikube dockerd[1328]: time="2024-12-02T20:01:48.652909951Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:01:50 minikube dockerd[1328]: time="2024-12-02T20:01:50.556558278Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:01:50 minikube dockerd[1328]: time="2024-12-02T20:01:50.556768690Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:01:52 minikube dockerd[1328]: time="2024-12-02T20:01:52.446868213Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:01:52 minikube dockerd[1328]: time="2024-12-02T20:01:52.446959418Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:02:14 minikube dockerd[1328]: time="2024-12-02T20:02:14.518854950Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:02:14 minikube dockerd[1328]: time="2024-12-02T20:02:14.519035760Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:02:16 minikube dockerd[1328]: time="2024-12-02T20:02:16.344672885Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:02:16 minikube dockerd[1328]: time="2024-12-02T20:02:16.344740289Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:02:18 minikube dockerd[1328]: time="2024-12-02T20:02:18.269213040Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:02:18 minikube dockerd[1328]: time="2024-12-02T20:02:18.269295445Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:03:01 minikube dockerd[1328]: time="2024-12-02T20:03:01.778274952Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:03:01 minikube dockerd[1328]: time="2024-12-02T20:03:01.778358756Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:03:08 minikube dockerd[1328]: time="2024-12-02T20:03:08.633927737Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:03:08 minikube dockerd[1328]: time="2024-12-02T20:03:08.633999541Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:03:12 minikube dockerd[1328]: time="2024-12-02T20:03:12.262215764Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:03:12 minikube dockerd[1328]: time="2024-12-02T20:03:12.262291968Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:04:28 minikube dockerd[1328]: time="2024-12-02T20:04:28.761966949Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:04:28 minikube dockerd[1328]: time="2024-12-02T20:04:28.762040453Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:04:30 minikube dockerd[1328]: time="2024-12-02T20:04:30.632988402Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:04:30 minikube dockerd[1328]: time="2024-12-02T20:04:30.633215716Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:04:46 minikube dockerd[1328]: time="2024-12-02T20:04:46.665438591Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:04:46 minikube dockerd[1328]: time="2024-12-02T20:04:46.670695074Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:07:19 minikube dockerd[1328]: time="2024-12-02T20:07:19.553949880Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:07:19 minikube dockerd[1328]: time="2024-12-02T20:07:19.554134117Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:07:21 minikube dockerd[1328]: time="2024-12-02T20:07:21.455879607Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:07:21 minikube dockerd[1328]: time="2024-12-02T20:07:21.455989329Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:07:35 minikube dockerd[1328]: time="2024-12-02T20:07:35.472039465Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:07:35 minikube dockerd[1328]: time="2024-12-02T20:07:35.472163797Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:12:30 minikube dockerd[1328]: time="2024-12-02T20:12:30.975216690Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:12:30 minikube dockerd[1328]: time="2024-12-02T20:12:30.975312890Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:12:33 minikube dockerd[1328]: time="2024-12-02T20:12:33.478668341Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:12:33 minikube dockerd[1328]: time="2024-12-02T20:12:33.479635799Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:12:47 minikube dockerd[1328]: time="2024-12-02T20:12:47.659007970Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:12:47 minikube dockerd[1328]: time="2024-12-02T20:12:47.659143572Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:17:46 minikube dockerd[1328]: time="2024-12-02T20:17:46.633675278Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:17:46 minikube dockerd[1328]: time="2024-12-02T20:17:46.633798385Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:17:48 minikube dockerd[1328]: time="2024-12-02T20:17:48.471591433Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:17:48 minikube dockerd[1328]: time="2024-12-02T20:17:48.471797446Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:17:50 minikube dockerd[1328]: time="2024-12-02T20:17:50.504362147Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:17:50 minikube dockerd[1328]: time="2024-12-02T20:17:50.504567760Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:22:53 minikube dockerd[1328]: time="2024-12-02T20:22:53.536171273Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:22:53 minikube dockerd[1328]: time="2024-12-02T20:22:53.537521610Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:22:56 minikube dockerd[1328]: time="2024-12-02T20:22:56.337872147Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:22:56 minikube dockerd[1328]: time="2024-12-02T20:22:56.338104970Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:22:59 minikube dockerd[1328]: time="2024-12-02T20:22:59.447500375Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:22:59 minikube dockerd[1328]: time="2024-12-02T20:22:59.447556881Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:28:04 minikube dockerd[1328]: time="2024-12-02T20:28:04.662976121Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:28:04 minikube dockerd[1328]: time="2024-12-02T20:28:04.663060725Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:28:06 minikube dockerd[1328]: time="2024-12-02T20:28:06.493944172Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:28:06 minikube dockerd[1328]: time="2024-12-02T20:28:06.494083778Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 02 20:28:09 minikube dockerd[1328]: time="2024-12-02T20:28:09.331525077Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 02 20:28:09 minikube dockerd[1328]: time="2024-12-02T20:28:09.331807189Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
ab7757e872d6c       6e38f40d628db       52 minutes ago      Running             storage-provisioner       0                   9762cb72385c0       storage-provisioner
298976039b2df       cbb01a7bd410d       52 minutes ago      Running             coredns                   0                   9518ed73774f9       coredns-6f6b679f8f-tp27d
a287be7474503       ad83b2ca7b09e       53 minutes ago      Running             kube-proxy                0                   94b010c94f74d       kube-proxy-v7m9n
9139b21b47c23       1766f54c897f0       53 minutes ago      Running             kube-scheduler            0                   001a085dc6d07       kube-scheduler-minikube
ae5f913f69193       604f5db92eaa8       53 minutes ago      Running             kube-apiserver            0                   7d12868f23af0       kube-apiserver-minikube
aac9a62fbfe78       045733566833c       53 minutes ago      Running             kube-controller-manager   0                   4135c91e42b0d       kube-controller-manager-minikube
7f889616cf801       2e96e5913fc06       53 minutes ago      Running             etcd                      0                   81dc9fbf339c6       etcd-minikube


==> coredns [298976039b2d] <==
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:42842 - 21977 "HINFO IN 7091431378620596326.605964530317535495. udp 56 false 512" NXDOMAIN qr,rd,ra 56 0.108680592s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_12_02T20_35_42_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 02 Dec 2024 19:35:33 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 02 Dec 2024 20:28:50 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 02 Dec 2024 20:26:59 +0000   Mon, 02 Dec 2024 19:35:31 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 02 Dec 2024 20:26:59 +0000   Mon, 02 Dec 2024 19:35:31 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 02 Dec 2024 20:26:59 +0000   Mon, 02 Dec 2024 19:35:31 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 02 Dec 2024 20:26:59 +0000   Mon, 02 Dec 2024 19:35:34 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3953540Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3953540Ki
  pods:               110
System Info:
  Machine ID:                 e5f718fbf3224bc0853aaab28bacf3fd
  System UUID:                e5f718fbf3224bc0853aaab28bacf3fd
  Boot ID:                    00b45089-5df4-4177-aed1-e02b8552b69b
  Kernel Version:             5.15.153.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     ionic-app-b4757dc8b-8z2wl           0 (0%)        0 (0%)      0 (0%)           0 (0%)         27m
  default                     ionic-app-b4757dc8b-qrfht           0 (0%)        0 (0%)      0 (0%)           0 (0%)         27m
  default                     ionic-app-b4757dc8b-rxd42           0 (0%)        0 (0%)      0 (0%)           0 (0%)         27m
  kube-system                 coredns-6f6b679f8f-tp27d            100m (2%)     0 (0%)      70Mi (1%)        170Mi (4%)     53m
  kube-system                 etcd-minikube                       100m (2%)     0 (0%)      100Mi (2%)       0 (0%)         53m
  kube-system                 kube-apiserver-minikube             250m (6%)     0 (0%)      0 (0%)           0 (0%)         53m
  kube-system                 kube-controller-manager-minikube    200m (5%)     0 (0%)      0 (0%)           0 (0%)         53m
  kube-system                 kube-proxy-v7m9n                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         53m
  kube-system                 kube-scheduler-minikube             100m (2%)     0 (0%)      0 (0%)           0 (0%)         53m
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         52m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%)  0 (0%)
  memory             170Mi (4%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           52m                kube-proxy       
  Warning  PossibleMemoryBackedVolumesOnDisk  53m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           53m                kubelet          Starting kubelet.
  Warning  CgroupV1                           53m                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   NodeAllocatableEnforced            53m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            53m (x7 over 53m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              53m (x7 over 53m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               53m (x7 over 53m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Warning  PossibleMemoryBackedVolumesOnDisk  53m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           53m                kubelet          Starting kubelet.
  Warning  CgroupV1                           53m                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   NodeAllocatableEnforced            53m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            53m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              53m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               53m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode                     53m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Dec 2 12:10] MDS CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html for more details.
[  +0.000000] TAA CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/tsx_async_abort.html for more details.
[  +0.000000] MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.
[  +0.000000]  #2 #3
[  +0.075392] PCI: Fatal: No config space access function found
[  +0.140006] PCI: System does not support PCI
[  +0.073349] kvm: no hardware support
[  +0.000010] kvm: no hardware support
[ +14.768750] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000008]  failed 2
[  +0.067864] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Budapest not found. Is the tzdata package installed?
[  +4.757893] misc dxg: dxgk: dxgglobal_acquire_channel_lock: Failed to acquire global channel lock
[  +0.901329] WSL (2) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.109857] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.096129] WSL (1) ERROR: ConfigMountFsTab:2589: Processing fstab with mount -a failed.
[  +0.105467] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000006]  failed 2
[  +0.052655] WSL (3) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.001414] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.003007] WSL (4) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.014388] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.058454] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Budapest not found. Is the tzdata package installed?
[  +0.483069] misc dxg: dxgk: dxgglobal_acquire_channel_lock: Failed to acquire global channel lock
[  +3.495344] netlink: 'init': attribute type 4 has an invalid length.
[Dec 2 16:29] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.031549] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +4.971295] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.007609] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[ +14.781525] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.004290] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[Dec 2 16:30] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.033603] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[Dec 2 17:06] hrtimer: interrupt took 31057343 ns
[Dec 2 17:07] tmpfs: Unknown parameter 'noswap'
[Dec 2 17:08] tmpfs: Unknown parameter 'noswap'
[Dec 2 19:35] tmpfs: Unknown parameter 'noswap'
[ +22.382207] tmpfs: Unknown parameter 'noswap'


==> etcd [7f889616cf80] <==
{"level":"info","ts":"2024-12-02T19:58:20.533760Z","caller":"traceutil/trace.go:171","msg":"trace[1993654715] range","detail":"{range_begin:/registry/limitranges/; range_end:/registry/limitranges0; response_count:0; response_revision:1507; }","duration":"595.86908ms","start":"2024-12-02T19:58:19.937798Z","end":"2024-12-02T19:58:20.533667Z","steps":["trace[1993654715] 'count revisions from in-memory index tree'  (duration: 595.74837ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-02T19:58:20.537464Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-02T19:58:19.937747Z","time spent":"599.666605ms","remote":"127.0.0.1:50168","response type":"/etcdserverpb.KV/Range","request count":0,"request size":50,"response count":0,"response size":29,"request content":"key:\"/registry/limitranges/\" range_end:\"/registry/limitranges0\" count_only:true "}
{"level":"info","ts":"2024-12-02T20:00:28.849825Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1370}
{"level":"info","ts":"2024-12-02T20:00:28.944564Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":1370,"took":"90.241125ms","hash":3934016646,"current-db-size-bytes":2031616,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1454080,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-12-02T20:00:28.944687Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3934016646,"revision":1370,"compact-revision":1134}
{"level":"info","ts":"2024-12-02T20:00:29.227975Z","caller":"traceutil/trace.go:171","msg":"trace[1633055970] transaction","detail":"{read_only:false; response_revision:1610; number_of_response:1; }","duration":"125.240696ms","start":"2024-12-02T20:00:29.102702Z","end":"2024-12-02T20:00:29.227943Z","steps":["trace[1633055970] 'process raft request'  (duration: 112.368408ms)","trace[1633055970] 'compare'  (duration: 12.72278ms)"],"step_count":2}
{"level":"info","ts":"2024-12-02T20:02:30.689632Z","caller":"traceutil/trace.go:171","msg":"trace[1193147412] transaction","detail":"{read_only:false; response_revision:1790; number_of_response:1; }","duration":"131.347596ms","start":"2024-12-02T20:02:30.557853Z","end":"2024-12-02T20:02:30.689201Z","steps":["trace[1193147412] 'process raft request'  (duration: 130.737961ms)"],"step_count":1}
{"level":"info","ts":"2024-12-02T20:02:57.880637Z","caller":"traceutil/trace.go:171","msg":"trace[129795378] transaction","detail":"{read_only:false; response_revision:1824; number_of_response:1; }","duration":"100.365569ms","start":"2024-12-02T20:02:57.780245Z","end":"2024-12-02T20:02:57.880610Z","steps":["trace[129795378] 'process raft request'  (duration: 100.145995ms)"],"step_count":1}
{"level":"info","ts":"2024-12-02T20:04:03.840287Z","caller":"traceutil/trace.go:171","msg":"trace[1494967597] transaction","detail":"{read_only:false; response_revision:1903; number_of_response:1; }","duration":"101.587355ms","start":"2024-12-02T20:04:03.738672Z","end":"2024-12-02T20:04:03.840259Z","steps":["trace[1494967597] 'process raft request'  (duration: 21.4352ms)","trace[1494967597] 'compare'  (duration: 80.055448ms)"],"step_count":2}
{"level":"warn","ts":"2024-12-02T20:04:24.582439Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"146.219527ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-12-02T20:04:24.583148Z","caller":"traceutil/trace.go:171","msg":"trace[1320223981] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1924; }","duration":"146.354035ms","start":"2024-12-02T20:04:24.436177Z","end":"2024-12-02T20:04:24.582531Z","steps":["trace[1320223981] 'agreement among raft nodes before linearized reading'  (duration: 146.161924ms)"],"step_count":1}
{"level":"info","ts":"2024-12-02T20:04:24.584975Z","caller":"traceutil/trace.go:171","msg":"trace[170205777] linearizableReadLoop","detail":"{readStateIndex:2286; appliedIndex:2285; }","duration":"145.974912ms","start":"2024-12-02T20:04:24.436185Z","end":"2024-12-02T20:04:24.582160Z","steps":["trace[170205777] 'read index received'  (duration: 111.700643ms)","trace[170205777] 'applied index is now lower than readState.Index'  (duration: 34.272969ms)"],"step_count":2}
{"level":"info","ts":"2024-12-02T20:05:28.845495Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1609}
{"level":"info","ts":"2024-12-02T20:05:28.861240Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":1609,"took":"15.394552ms","hash":2166031061,"current-db-size-bytes":2031616,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1773568,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-12-02T20:05:28.861334Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2166031061,"revision":1609,"compact-revision":1370}
{"level":"warn","ts":"2024-12-02T20:06:29.325485Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"190.089535ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033643560140339 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:2021 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128033643560140337 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2024-12-02T20:06:29.325637Z","caller":"traceutil/trace.go:171","msg":"trace[1679715045] transaction","detail":"{read_only:false; response_revision:2030; number_of_response:1; }","duration":"290.871798ms","start":"2024-12-02T20:06:29.034737Z","end":"2024-12-02T20:06:29.325609Z","steps":["trace[1679715045] 'process raft request'  (duration: 20.985191ms)","trace[1679715045] 'compare'  (duration: 98.076406ms)","trace[1679715045] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/masterleases/192.168.49.2; req_size:115; } (duration: 52.768007ms)"],"step_count":3}
{"level":"info","ts":"2024-12-02T20:07:58.255592Z","caller":"traceutil/trace.go:171","msg":"trace[1693427627] transaction","detail":"{read_only:false; response_revision:2108; number_of_response:1; }","duration":"109.664232ms","start":"2024-12-02T20:07:58.145907Z","end":"2024-12-02T20:07:58.255571Z","steps":["trace[1693427627] 'process raft request'  (duration: 109.252796ms)"],"step_count":1}
{"level":"info","ts":"2024-12-02T20:10:06.998659Z","caller":"traceutil/trace.go:171","msg":"trace[2073701490] transaction","detail":"{read_only:false; response_revision:2211; number_of_response:1; }","duration":"120.712059ms","start":"2024-12-02T20:10:06.877922Z","end":"2024-12-02T20:10:06.998634Z","steps":["trace[2073701490] 'process raft request'  (duration: 120.522646ms)"],"step_count":1}
{"level":"info","ts":"2024-12-02T20:10:28.794106Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1981}
{"level":"info","ts":"2024-12-02T20:10:28.809639Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":1981,"took":"13.745565ms","hash":653738374,"current-db-size-bytes":2031616,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1871872,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2024-12-02T20:10:28.809727Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":653738374,"revision":1981,"compact-revision":1609}
{"level":"info","ts":"2024-12-02T20:12:26.555637Z","caller":"traceutil/trace.go:171","msg":"trace[2123530135] linearizableReadLoop","detail":"{readStateIndex:2789; appliedIndex:2788; }","duration":"219.255694ms","start":"2024-12-02T20:12:26.336032Z","end":"2024-12-02T20:12:26.555288Z","steps":["trace[2123530135] 'read index received'  (duration: 219.041644ms)","trace[2123530135] 'applied index is now lower than readState.Index'  (duration: 213.45µs)"],"step_count":2}
{"level":"info","ts":"2024-12-02T20:12:26.556670Z","caller":"traceutil/trace.go:171","msg":"trace[1111044599] transaction","detail":"{read_only:false; response_revision:2327; number_of_response:1; }","duration":"292.631628ms","start":"2024-12-02T20:12:26.264009Z","end":"2024-12-02T20:12:26.556640Z","steps":["trace[1111044599] 'process raft request'  (duration: 291.141178ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-02T20:12:26.560374Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"122.568187ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/runtimeclasses/\" range_end:\"/registry/runtimeclasses0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-12-02T20:12:26.560497Z","caller":"traceutil/trace.go:171","msg":"trace[559153545] range","detail":"{range_begin:/registry/runtimeclasses/; range_end:/registry/runtimeclasses0; response_count:0; response_revision:2327; }","duration":"122.736026ms","start":"2024-12-02T20:12:26.437689Z","end":"2024-12-02T20:12:26.560425Z","steps":["trace[559153545] 'agreement among raft nodes before linearized reading'  (duration: 122.097676ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-02T20:12:26.560996Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"122.864256ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-12-02T20:12:26.561126Z","caller":"traceutil/trace.go:171","msg":"trace[1903431330] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:2327; }","duration":"123.004489ms","start":"2024-12-02T20:12:26.438099Z","end":"2024-12-02T20:12:26.561104Z","steps":["trace[1903431330] 'agreement among raft nodes before linearized reading'  (duration: 122.829048ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-02T20:12:26.565357Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"229.333861ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-12-02T20:12:26.565433Z","caller":"traceutil/trace.go:171","msg":"trace[21879929] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:2327; }","duration":"229.426883ms","start":"2024-12-02T20:12:26.335989Z","end":"2024-12-02T20:12:26.565416Z","steps":["trace[21879929] 'agreement among raft nodes before linearized reading'  (duration: 220.791655ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-02T20:12:35.267000Z","caller":"etcdserver/v3_server.go:920","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128033643560142180,"retry-timeout":"500ms"}
{"level":"warn","ts":"2024-12-02T20:12:35.774160Z","caller":"etcdserver/v3_server.go:920","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128033643560142180,"retry-timeout":"500ms"}
{"level":"warn","ts":"2024-12-02T20:12:36.093184Z","caller":"wal/wal.go:805","msg":"slow fdatasync","took":"1.351320855s","expected-duration":"1s"}
{"level":"info","ts":"2024-12-02T20:12:36.100094Z","caller":"traceutil/trace.go:171","msg":"trace[2068861856] linearizableReadLoop","detail":"{readStateIndex:2798; appliedIndex:2797; }","duration":"1.333256866s","start":"2024-12-02T20:12:34.766806Z","end":"2024-12-02T20:12:36.100063Z","steps":["trace[2068861856] 'read index received'  (duration: 1.333030652s)","trace[2068861856] 'applied index is now lower than readState.Index'  (duration: 225.314µs)"],"step_count":2}
{"level":"warn","ts":"2024-12-02T20:12:36.102010Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.021651783s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/\" range_end:\"/registry/serviceaccounts0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-12-02T20:12:36.102177Z","caller":"traceutil/trace.go:171","msg":"trace[1737382014] range","detail":"{range_begin:/registry/serviceaccounts/; range_end:/registry/serviceaccounts0; response_count:0; response_revision:2334; }","duration":"1.021842095s","start":"2024-12-02T20:12:35.080308Z","end":"2024-12-02T20:12:36.102150Z","steps":["trace[1737382014] 'agreement among raft nodes before linearized reading'  (duration: 1.02144227s)"],"step_count":1}
{"level":"warn","ts":"2024-12-02T20:12:36.103094Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"660.118991ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-12-02T20:12:36.103375Z","caller":"traceutil/trace.go:171","msg":"trace[1417746226] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:2334; }","duration":"660.367806ms","start":"2024-12-02T20:12:35.442945Z","end":"2024-12-02T20:12:36.103313Z","steps":["trace[1417746226] 'agreement among raft nodes before linearized reading'  (duration: 660.090189ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-02T20:12:36.103155Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.336267047s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/validatingwebhookconfigurations/\" range_end:\"/registry/validatingwebhookconfigurations0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-12-02T20:12:36.103699Z","caller":"traceutil/trace.go:171","msg":"trace[1109013702] range","detail":"{range_begin:/registry/validatingwebhookconfigurations/; range_end:/registry/validatingwebhookconfigurations0; response_count:0; response_revision:2334; }","duration":"1.336878884s","start":"2024-12-02T20:12:34.766796Z","end":"2024-12-02T20:12:36.103675Z","steps":["trace[1109013702] 'agreement among raft nodes before linearized reading'  (duration: 1.336232745s)"],"step_count":1}
{"level":"info","ts":"2024-12-02T20:12:36.100436Z","caller":"traceutil/trace.go:171","msg":"trace[639250420] transaction","detail":"{read_only:false; response_revision:2334; number_of_response:1; }","duration":"1.358694999s","start":"2024-12-02T20:12:34.741718Z","end":"2024-12-02T20:12:36.100413Z","steps":["trace[639250420] 'process raft request'  (duration: 1.358193169s)"],"step_count":1}
{"level":"warn","ts":"2024-12-02T20:12:36.106519Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-02T20:12:35.080208Z","time spent":"1.022008805s","remote":"127.0.0.1:50232","response type":"/etcdserverpb.KV/Range","request count":0,"request size":58,"response count":41,"response size":31,"request content":"key:\"/registry/serviceaccounts/\" range_end:\"/registry/serviceaccounts0\" count_only:true "}
{"level":"warn","ts":"2024-12-02T20:12:36.117998Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-02T20:12:34.766761Z","time spent":"1.337050795s","remote":"127.0.0.1:50520","response type":"/etcdserverpb.KV/Range","request count":0,"request size":90,"response count":0,"response size":29,"request content":"key:\"/registry/validatingwebhookconfigurations/\" range_end:\"/registry/validatingwebhookconfigurations0\" count_only:true "}
{"level":"warn","ts":"2024-12-02T20:12:36.109525Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-02T20:12:34.741691Z","time spent":"1.363820808s","remote":"127.0.0.1:50190","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:2332 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-12-02T20:12:46.462607Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"110.354771ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-12-02T20:12:46.462706Z","caller":"traceutil/trace.go:171","msg":"trace[870663575] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:2344; }","duration":"110.470673ms","start":"2024-12-02T20:12:46.352215Z","end":"2024-12-02T20:12:46.462685Z","steps":["trace[870663575] 'range keys from in-memory index tree'  (duration: 110.287271ms)"],"step_count":1}
{"level":"info","ts":"2024-12-02T20:14:12.281258Z","caller":"traceutil/trace.go:171","msg":"trace[1853703121] transaction","detail":"{read_only:false; response_revision:2415; number_of_response:1; }","duration":"746.711077ms","start":"2024-12-02T20:14:11.534519Z","end":"2024-12-02T20:14:12.281231Z","steps":["trace[1853703121] 'process raft request'  (duration: 746.570569ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-02T20:14:12.281427Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-02T20:14:11.534490Z","time spent":"746.851484ms","remote":"127.0.0.1:50190","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:2414 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-12-02T20:15:28.817506Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2229}
{"level":"info","ts":"2024-12-02T20:15:28.850137Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":2229,"took":"28.670609ms","hash":2603649918,"current-db-size-bytes":2031616,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1630208,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-12-02T20:15:28.850223Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2603649918,"revision":2229,"compact-revision":1981}
{"level":"info","ts":"2024-12-02T20:16:39.212086Z","caller":"traceutil/trace.go:171","msg":"trace[1721744948] transaction","detail":"{read_only:false; response_revision:2535; number_of_response:1; }","duration":"135.422473ms","start":"2024-12-02T20:16:39.076631Z","end":"2024-12-02T20:16:39.212053Z","steps":["trace[1721744948] 'process raft request'  (duration: 103.335568ms)","trace[1721744948] 'compare'  (duration: 31.760679ms)"],"step_count":2}
{"level":"info","ts":"2024-12-02T20:20:15.312671Z","caller":"traceutil/trace.go:171","msg":"trace[1034884116] transaction","detail":"{read_only:false; response_revision:2713; number_of_response:1; }","duration":"101.178521ms","start":"2024-12-02T20:20:15.211464Z","end":"2024-12-02T20:20:15.312643Z","steps":["trace[1034884116] 'process raft request'  (duration: 100.893906ms)"],"step_count":1}
{"level":"info","ts":"2024-12-02T20:20:28.834992Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2476}
{"level":"info","ts":"2024-12-02T20:20:28.868025Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":2476,"took":"22.824085ms","hash":1748863499,"current-db-size-bytes":2031616,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1634304,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-12-02T20:20:28.868192Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1748863499,"revision":2476,"compact-revision":2229}
{"level":"info","ts":"2024-12-02T20:25:28.816145Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2723}
{"level":"info","ts":"2024-12-02T20:25:28.823215Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":2723,"took":"6.318923ms","hash":2288154185,"current-db-size-bytes":2031616,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1658880,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-12-02T20:25:28.823296Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2288154185,"revision":2723,"compact-revision":2476}
{"level":"info","ts":"2024-12-02T20:28:03.067979Z","caller":"traceutil/trace.go:171","msg":"trace[2036766360] transaction","detail":"{read_only:false; response_revision:3098; number_of_response:1; }","duration":"166.323963ms","start":"2024-12-02T20:28:02.901626Z","end":"2024-12-02T20:28:03.067950Z","steps":["trace[2036766360] 'process raft request'  (duration: 166.190157ms)"],"step_count":1}


==> kernel <==
 20:28:52 up  8:18,  0 users,  load average: 1.23, 1.10, 1.49
Linux minikube 5.15.153.1-microsoft-standard-WSL2 #1 SMP Fri Mar 29 23:14:13 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [ae5f913f6919] <==
I1202 19:35:33.037853       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I1202 19:35:33.038042       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1202 19:35:33.038130       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1202 19:35:33.038230       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1202 19:35:33.038385       1 aggregator.go:169] waiting for initial CRD sync...
I1202 19:35:33.059866       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1202 19:35:33.064893       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1202 19:35:33.065350       1 controller.go:119] Starting legacy_token_tracking_controller
I1202 19:35:33.065648       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I1202 19:35:33.075644       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I1202 19:35:33.075752       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I1202 19:35:33.075843       1 local_available_controller.go:156] Starting LocalAvailability controller
I1202 19:35:33.075892       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1202 19:35:33.076029       1 controller.go:142] Starting OpenAPI controller
I1202 19:35:33.076126       1 controller.go:90] Starting OpenAPI V3 controller
I1202 19:35:33.076193       1 naming_controller.go:294] Starting NamingConditionController
I1202 19:35:33.076251       1 establishing_controller.go:81] Starting EstablishingController
I1202 19:35:33.076375       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1202 19:35:33.076477       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1202 19:35:33.076560       1 crd_finalizer.go:269] Starting CRDFinalizer
I1202 19:35:33.076623       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1202 19:35:33.076672       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I1202 19:35:33.077373       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1202 19:35:33.077656       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1202 19:35:33.368051       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1202 19:35:33.368576       1 shared_informer.go:320] Caches are synced for configmaps
I1202 19:35:33.371385       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1202 19:35:33.382540       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1202 19:35:33.382668       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1202 19:35:33.384827       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1202 19:35:33.385145       1 cache.go:39] Caches are synced for LocalAvailability controller
I1202 19:35:33.385467       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1202 19:35:33.442865       1 shared_informer.go:320] Caches are synced for node_authorizer
I1202 19:35:33.442985       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I1202 19:35:33.444113       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1202 19:35:33.444471       1 policy_source.go:224] refreshing policies
I1202 19:35:33.451032       1 aggregator.go:171] initial CRD sync complete...
I1202 19:35:33.451393       1 autoregister_controller.go:144] Starting autoregister controller
I1202 19:35:33.451638       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1202 19:35:33.451877       1 cache.go:39] Caches are synced for autoregister controller
I1202 19:35:33.552569       1 controller.go:615] quota admission added evaluator for: namespaces
E1202 19:35:33.672026       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
E1202 19:35:33.698514       1 controller.go:148] "Unhandled Error" err="while syncing ConfigMap \"kube-system/kube-apiserver-legacy-service-account-token-tracking\", err: namespaces \"kube-system\" not found" logger="UnhandledError"
I1202 19:35:33.981414       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I1202 19:35:34.089482       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I1202 19:35:34.162045       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I1202 19:35:34.162088       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1202 19:35:38.488363       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1202 19:35:38.627361       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1202 19:35:38.995360       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W1202 19:35:39.024949       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I1202 19:35:39.044740       1 controller.go:615] quota admission added evaluator for: endpoints
I1202 19:35:39.114974       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1202 19:35:39.438367       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I1202 19:35:40.540340       1 controller.go:615] quota admission added evaluator for: deployments.apps
I1202 19:35:40.738385       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I1202 19:35:40.805203       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I1202 19:35:44.774240       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I1202 19:35:45.124793       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I1202 20:04:20.756000       1 alloc.go:330] "allocated clusterIPs" service="default/ionic-app-service" clusterIPs={"IPv4":"10.100.129.104"}


==> kube-controller-manager [aac9a62fbfe7] <==
I1202 20:01:48.745118       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="57.604µs"
I1202 20:02:00.770950       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="112.206µs"
I1202 20:02:02.785591       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="46.502µs"
I1202 20:02:02.828569       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="69.104µs"
I1202 20:02:12.749402       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="49.903µs"
I1202 20:02:13.729441       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="94.505µs"
I1202 20:02:15.740698       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="64.404µs"
I1202 20:02:28.757986       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="60.004µs"
I1202 20:02:28.776303       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="104.306µs"
I1202 20:02:29.757839       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="75.105µs"
I1202 20:02:40.725688       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="58.803µs"
I1202 20:02:42.748002       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="65.803µs"
I1202 20:02:44.763102       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="71.103µs"
I1202 20:03:12.792471       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="111.306µs"
I1202 20:03:22.747346       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="129.915µs"
I1202 20:03:26.738154       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="64.404µs"
I1202 20:03:26.759804       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="60.104µs"
I1202 20:03:35.728166       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="73.883µs"
I1202 20:03:37.762402       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="67.985µs"
I1202 20:04:39.744062       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="114µs"
I1202 20:04:44.750542       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="76.899µs"
I1202 20:04:54.744427       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="71.703µs"
I1202 20:04:57.724970       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="71.214µs"
I1202 20:05:01.732724       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="86.204µs"
I1202 20:05:14.746763       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="95.109µs"
I1202 20:06:33.611021       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1202 20:07:30.684936       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="587.553µs"
I1202 20:07:36.673561       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="50.914µs"
I1202 20:07:43.647339       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="65.317µs"
I1202 20:07:49.663502       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="61.416µs"
I1202 20:07:51.644790       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="112.537µs"
I1202 20:08:04.672092       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="90.52µs"
I1202 20:11:39.746094       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1202 20:12:43.630765       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="169.602µs"
I1202 20:12:45.666246       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="64.701µs"
I1202 20:12:56.715216       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="65.514µs"
I1202 20:13:00.632205       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="926.94µs"
I1202 20:13:01.628084       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="132.306µs"
I1202 20:13:13.654544       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="86.598µs"
I1202 20:16:46.935079       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1202 20:17:58.604230       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="194.014µs"
I1202 20:18:01.613918       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="95.84µs"
I1202 20:18:03.603430       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="116.448µs"
I1202 20:18:10.601375       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="65.304µs"
I1202 20:18:13.604338       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="87.305µs"
I1202 20:18:18.597924       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="113.802µs"
I1202 20:21:53.492019       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1202 20:23:05.594059       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="715.377µs"
I1202 20:23:10.581658       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="154.216µs"
I1202 20:23:13.570583       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="362.639µs"
I1202 20:23:19.591191       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="59.606µs"
I1202 20:23:23.577713       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="41.904µs"
I1202 20:23:26.565923       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="742.475µs"
I1202 20:26:59.777963       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1202 20:28:18.545914       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="1.695475ms"
I1202 20:28:20.538360       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="106.105µs"
I1202 20:28:23.528067       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="60.602µs"
I1202 20:28:32.553382       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="145.708µs"
I1202 20:28:33.526925       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="59.403µs"
I1202 20:28:36.559123       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ionic-app-b4757dc8b" duration="53.102µs"


==> kube-proxy [a287be747450] <==
E1202 19:35:56.367192       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E1202 19:35:56.382810       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I1202 19:35:56.738651       1 server_linux.go:66] "Using iptables proxy"
I1202 19:35:57.793522       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1202 19:35:57.793715       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1202 19:35:57.987083       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1202 19:35:57.987208       1 server_linux.go:169] "Using iptables Proxier"
I1202 19:35:57.992524       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E1202 19:35:58.023798       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E1202 19:35:58.052601       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I1202 19:35:58.052842       1 server.go:483] "Version info" version="v1.31.0"
I1202 19:35:58.052863       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1202 19:35:58.068590       1 config.go:197] "Starting service config controller"
I1202 19:35:58.073615       1 config.go:326] "Starting node config controller"
I1202 19:35:58.081770       1 shared_informer.go:313] Waiting for caches to sync for service config
I1202 19:35:58.083627       1 shared_informer.go:313] Waiting for caches to sync for node config
I1202 19:35:58.083772       1 config.go:104] "Starting endpoint slice config controller"
I1202 19:35:58.083788       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1202 19:35:58.182368       1 shared_informer.go:320] Caches are synced for service config
I1202 19:35:58.184795       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1202 19:35:58.184828       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [9139b21b47c2] <==
E1202 19:35:33.786619       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1202 19:35:33.786884       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1202 19:35:33.786921       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1202 19:35:33.787100       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1202 19:35:33.787132       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1202 19:35:33.787271       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1202 19:35:33.787305       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1202 19:35:33.787564       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1202 19:35:33.787605       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1202 19:35:33.791525       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1202 19:35:33.791645       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1202 19:35:33.793290       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1202 19:35:33.793387       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1202 19:35:34.620401       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1202 19:35:34.620489       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1202 19:35:34.733520       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1202 19:35:34.733707       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1202 19:35:34.767695       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1202 19:35:34.768393       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1202 19:35:34.786975       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1202 19:35:34.787065       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W1202 19:35:34.844409       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1202 19:35:34.844739       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1202 19:35:34.876820       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1202 19:35:34.877283       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1202 19:35:35.012141       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1202 19:35:35.019725       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1202 19:35:35.022918       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1202 19:35:35.023055       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1202 19:35:35.031844       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1202 19:35:35.032113       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1202 19:35:35.060753       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1202 19:35:35.061375       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1202 19:35:35.063346       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1202 19:35:35.063410       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1202 19:35:35.179415       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1202 19:35:35.179916       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1202 19:35:35.211332       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1202 19:35:35.213066       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1202 19:35:35.241977       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1202 19:35:35.242057       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1202 19:35:35.299974       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1202 19:35:35.300435       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1202 19:35:36.496936       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1202 19:35:36.497019       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1202 19:35:36.934039       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1202 19:35:36.934143       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1202 19:35:37.002395       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1202 19:35:37.002454       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1202 19:35:37.079902       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1202 19:35:37.080047       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1202 19:35:37.306579       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1202 19:35:37.306685       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1202 19:35:37.332138       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1202 19:35:37.332254       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1202 19:35:37.365231       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1202 19:35:37.365327       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1202 19:35:37.725170       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1202 19:35:37.725307       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
I1202 19:35:43.073731       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Dec 02 20:25:11 minikube kubelet[2537]: E1202 20:25:11.545057    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-8z2wl" podUID="20b6bb08-551b-4acb-beaa-7383e19c4fb5"
Dec 02 20:25:12 minikube kubelet[2537]: E1202 20:25:12.548005    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-rxd42" podUID="f9b137a5-ed11-4e8c-986a-1639e6a6be52"
Dec 02 20:25:20 minikube kubelet[2537]: E1202 20:25:20.546813    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-qrfht" podUID="ee5e6d53-1114-44a1-9a0b-f6770b85bb1a"
Dec 02 20:25:24 minikube kubelet[2537]: E1202 20:25:24.538039    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-rxd42" podUID="f9b137a5-ed11-4e8c-986a-1639e6a6be52"
Dec 02 20:25:25 minikube kubelet[2537]: E1202 20:25:25.536609    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-8z2wl" podUID="20b6bb08-551b-4acb-beaa-7383e19c4fb5"
Dec 02 20:25:34 minikube kubelet[2537]: E1202 20:25:34.539593    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-qrfht" podUID="ee5e6d53-1114-44a1-9a0b-f6770b85bb1a"
Dec 02 20:25:35 minikube kubelet[2537]: E1202 20:25:35.536929    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-rxd42" podUID="f9b137a5-ed11-4e8c-986a-1639e6a6be52"
Dec 02 20:25:40 minikube kubelet[2537]: E1202 20:25:40.542543    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-8z2wl" podUID="20b6bb08-551b-4acb-beaa-7383e19c4fb5"
Dec 02 20:25:48 minikube kubelet[2537]: E1202 20:25:48.538237    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-rxd42" podUID="f9b137a5-ed11-4e8c-986a-1639e6a6be52"
Dec 02 20:25:48 minikube kubelet[2537]: E1202 20:25:48.555844    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-qrfht" podUID="ee5e6d53-1114-44a1-9a0b-f6770b85bb1a"
Dec 02 20:25:54 minikube kubelet[2537]: E1202 20:25:54.528025    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-8z2wl" podUID="20b6bb08-551b-4acb-beaa-7383e19c4fb5"
Dec 02 20:26:02 minikube kubelet[2537]: E1202 20:26:02.525897    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-qrfht" podUID="ee5e6d53-1114-44a1-9a0b-f6770b85bb1a"
Dec 02 20:26:03 minikube kubelet[2537]: E1202 20:26:03.529759    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-rxd42" podUID="f9b137a5-ed11-4e8c-986a-1639e6a6be52"
Dec 02 20:26:08 minikube kubelet[2537]: E1202 20:26:08.523568    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-8z2wl" podUID="20b6bb08-551b-4acb-beaa-7383e19c4fb5"
Dec 02 20:26:15 minikube kubelet[2537]: E1202 20:26:15.524174    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-qrfht" podUID="ee5e6d53-1114-44a1-9a0b-f6770b85bb1a"
Dec 02 20:26:18 minikube kubelet[2537]: E1202 20:26:18.519583    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-rxd42" podUID="f9b137a5-ed11-4e8c-986a-1639e6a6be52"
Dec 02 20:26:19 minikube kubelet[2537]: E1202 20:26:19.519159    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-8z2wl" podUID="20b6bb08-551b-4acb-beaa-7383e19c4fb5"
Dec 02 20:26:30 minikube kubelet[2537]: E1202 20:26:30.527949    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-qrfht" podUID="ee5e6d53-1114-44a1-9a0b-f6770b85bb1a"
Dec 02 20:26:32 minikube kubelet[2537]: E1202 20:26:32.519655    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-8z2wl" podUID="20b6bb08-551b-4acb-beaa-7383e19c4fb5"
Dec 02 20:26:33 minikube kubelet[2537]: E1202 20:26:33.524347    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-rxd42" podUID="f9b137a5-ed11-4e8c-986a-1639e6a6be52"
Dec 02 20:26:44 minikube kubelet[2537]: E1202 20:26:44.521607    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-8z2wl" podUID="20b6bb08-551b-4acb-beaa-7383e19c4fb5"
Dec 02 20:26:44 minikube kubelet[2537]: E1202 20:26:44.521725    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-qrfht" podUID="ee5e6d53-1114-44a1-9a0b-f6770b85bb1a"
Dec 02 20:26:45 minikube kubelet[2537]: E1202 20:26:45.519106    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-rxd42" podUID="f9b137a5-ed11-4e8c-986a-1639e6a6be52"
Dec 02 20:26:56 minikube kubelet[2537]: E1202 20:26:56.517126    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-rxd42" podUID="f9b137a5-ed11-4e8c-986a-1639e6a6be52"
Dec 02 20:26:57 minikube kubelet[2537]: E1202 20:26:57.520836    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-8z2wl" podUID="20b6bb08-551b-4acb-beaa-7383e19c4fb5"
Dec 02 20:26:57 minikube kubelet[2537]: E1202 20:26:57.521010    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-qrfht" podUID="ee5e6d53-1114-44a1-9a0b-f6770b85bb1a"
Dec 02 20:27:08 minikube kubelet[2537]: E1202 20:27:08.518845    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-8z2wl" podUID="20b6bb08-551b-4acb-beaa-7383e19c4fb5"
Dec 02 20:27:10 minikube kubelet[2537]: E1202 20:27:10.518475    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-rxd42" podUID="f9b137a5-ed11-4e8c-986a-1639e6a6be52"
Dec 02 20:27:12 minikube kubelet[2537]: E1202 20:27:12.546720    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-qrfht" podUID="ee5e6d53-1114-44a1-9a0b-f6770b85bb1a"
Dec 02 20:27:19 minikube kubelet[2537]: E1202 20:27:19.516558    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-8z2wl" podUID="20b6bb08-551b-4acb-beaa-7383e19c4fb5"
Dec 02 20:27:24 minikube kubelet[2537]: E1202 20:27:24.540776    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-rxd42" podUID="f9b137a5-ed11-4e8c-986a-1639e6a6be52"
Dec 02 20:27:25 minikube kubelet[2537]: E1202 20:27:25.516302    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-qrfht" podUID="ee5e6d53-1114-44a1-9a0b-f6770b85bb1a"
Dec 02 20:27:30 minikube kubelet[2537]: E1202 20:27:30.516927    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-8z2wl" podUID="20b6bb08-551b-4acb-beaa-7383e19c4fb5"
Dec 02 20:27:38 minikube kubelet[2537]: E1202 20:27:38.515710    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-rxd42" podUID="f9b137a5-ed11-4e8c-986a-1639e6a6be52"
Dec 02 20:27:40 minikube kubelet[2537]: E1202 20:27:40.526993    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-qrfht" podUID="ee5e6d53-1114-44a1-9a0b-f6770b85bb1a"
Dec 02 20:27:44 minikube kubelet[2537]: E1202 20:27:44.517269    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-8z2wl" podUID="20b6bb08-551b-4acb-beaa-7383e19c4fb5"
Dec 02 20:27:51 minikube kubelet[2537]: E1202 20:27:51.516518    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-qrfht" podUID="ee5e6d53-1114-44a1-9a0b-f6770b85bb1a"
Dec 02 20:27:53 minikube kubelet[2537]: E1202 20:27:53.514689    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-rxd42" podUID="f9b137a5-ed11-4e8c-986a-1639e6a6be52"
Dec 02 20:27:55 minikube kubelet[2537]: E1202 20:27:55.515289    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-8z2wl" podUID="20b6bb08-551b-4acb-beaa-7383e19c4fb5"
Dec 02 20:28:04 minikube kubelet[2537]: E1202 20:28:04.674191    2537 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for ionic-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="ionic-app:latest"
Dec 02 20:28:04 minikube kubelet[2537]: E1202 20:28:04.674292    2537 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for ionic-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="ionic-app:latest"
Dec 02 20:28:04 minikube kubelet[2537]: E1202 20:28:04.674625    2537 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:ionic-app,Image:ionic-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:NODE_ENV,Value:production,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5zvvc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ionic-app-b4757dc8b-qrfht_default(ee5e6d53-1114-44a1-9a0b-f6770b85bb1a): ErrImagePull: Error response from daemon: pull access denied for ionic-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Dec 02 20:28:04 minikube kubelet[2537]: E1202 20:28:04.676067    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ErrImagePull: \"Error response from daemon: pull access denied for ionic-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/ionic-app-b4757dc8b-qrfht" podUID="ee5e6d53-1114-44a1-9a0b-f6770b85bb1a"
Dec 02 20:28:06 minikube kubelet[2537]: E1202 20:28:06.500076    2537 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for ionic-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="ionic-app:latest"
Dec 02 20:28:06 minikube kubelet[2537]: E1202 20:28:06.500155    2537 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for ionic-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="ionic-app:latest"
Dec 02 20:28:06 minikube kubelet[2537]: E1202 20:28:06.500316    2537 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:ionic-app,Image:ionic-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:NODE_ENV,Value:production,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v2hv4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ionic-app-b4757dc8b-rxd42_default(f9b137a5-ed11-4e8c-986a-1639e6a6be52): ErrImagePull: Error response from daemon: pull access denied for ionic-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Dec 02 20:28:06 minikube kubelet[2537]: E1202 20:28:06.501571    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ErrImagePull: \"Error response from daemon: pull access denied for ionic-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/ionic-app-b4757dc8b-rxd42" podUID="f9b137a5-ed11-4e8c-986a-1639e6a6be52"
Dec 02 20:28:09 minikube kubelet[2537]: E1202 20:28:09.351420    2537 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for ionic-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="ionic-app:latest"
Dec 02 20:28:09 minikube kubelet[2537]: E1202 20:28:09.351611    2537 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for ionic-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="ionic-app:latest"
Dec 02 20:28:09 minikube kubelet[2537]: E1202 20:28:09.353911    2537 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:ionic-app,Image:ionic-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:NODE_ENV,Value:production,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6tnhr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ionic-app-b4757dc8b-8z2wl_default(20b6bb08-551b-4acb-beaa-7383e19c4fb5): ErrImagePull: Error response from daemon: pull access denied for ionic-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Dec 02 20:28:09 minikube kubelet[2537]: E1202 20:28:09.355649    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ErrImagePull: \"Error response from daemon: pull access denied for ionic-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/ionic-app-b4757dc8b-8z2wl" podUID="20b6bb08-551b-4acb-beaa-7383e19c4fb5"
Dec 02 20:28:18 minikube kubelet[2537]: E1202 20:28:18.514445    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-qrfht" podUID="ee5e6d53-1114-44a1-9a0b-f6770b85bb1a"
Dec 02 20:28:20 minikube kubelet[2537]: E1202 20:28:20.514541    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-rxd42" podUID="f9b137a5-ed11-4e8c-986a-1639e6a6be52"
Dec 02 20:28:23 minikube kubelet[2537]: E1202 20:28:23.513824    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-8z2wl" podUID="20b6bb08-551b-4acb-beaa-7383e19c4fb5"
Dec 02 20:28:32 minikube kubelet[2537]: E1202 20:28:32.520938    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-qrfht" podUID="ee5e6d53-1114-44a1-9a0b-f6770b85bb1a"
Dec 02 20:28:33 minikube kubelet[2537]: E1202 20:28:33.513748    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-rxd42" podUID="f9b137a5-ed11-4e8c-986a-1639e6a6be52"
Dec 02 20:28:36 minikube kubelet[2537]: E1202 20:28:36.554018    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-8z2wl" podUID="20b6bb08-551b-4acb-beaa-7383e19c4fb5"
Dec 02 20:28:47 minikube kubelet[2537]: E1202 20:28:47.510785    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-qrfht" podUID="ee5e6d53-1114-44a1-9a0b-f6770b85bb1a"
Dec 02 20:28:48 minikube kubelet[2537]: E1202 20:28:48.510433    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-rxd42" podUID="f9b137a5-ed11-4e8c-986a-1639e6a6be52"
Dec 02 20:28:48 minikube kubelet[2537]: E1202 20:28:48.510597    2537 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ionic-app\" with ImagePullBackOff: \"Back-off pulling image \\\"ionic-app:latest\\\"\"" pod="default/ionic-app-b4757dc8b-8z2wl" podUID="20b6bb08-551b-4acb-beaa-7383e19c4fb5"


==> storage-provisioner [ab7757e872d6] <==
I1202 19:35:58.651003       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1202 19:35:58.711292       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1202 19:35:58.712095       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1202 19:35:58.730886       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1202 19:35:58.731035       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"6a03167e-495a-4b86-bb16-ea9a8ac4a7fd", APIVersion:"v1", ResourceVersion:"438", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_61da2d7d-3f04-4b35-a056-34e506461c4b became leader
I1202 19:35:58.731228       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_61da2d7d-3f04-4b35-a056-34e506461c4b!
I1202 19:35:58.833272       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_61da2d7d-3f04-4b35-a056-34e506461c4b!

